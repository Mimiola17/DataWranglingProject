{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reporting: wrangle_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Gathering\n",
    "\n",
    "The data used for the wrangling project was gathered from three sources.\n",
    "- The first which is a csv file of the WeRateDogs archive was downloaded manually from the link provided\n",
    "* The second file which contains the image prediction of the tweets was downloaded programmatically from the Udacity server using the request library. The content however had to be decoded to the common unicode encodings to enable easy reading of the tab delimited string output. This string was passed into StringIO which enabled pandas to be able to read the string and made it a dataframe\n",
    "- The third file was queried from Twitter API using the tweepy library to get download each tweet's retweet and favorite counts. This returned a json file which was accessed using the json library and then read into a pandas DataFrame."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accessing Data\n",
    "<ul>The data was accessed both Visually and Programmatically; 8 **Quality** issues and 2 **Tidiness** issues were identified. The visual access revealed lots of missing data in the archive csv file and inaccurate data in the image prediction file. Using the pandas inbuilt functions, other quality issues were identified programmatically.</ul>\n",
    "\n",
    "<ul>Below are the issues found in the data while accessing them:</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quality issues\n",
    "1. `tweet_archive Table`: remove the 181 retweeted tweets\n",
    "\n",
    "2. `tweet_archive Table`: remove retweeted related columns because they are empty after handling issue 1 above - 'retweeted_status_id', 'retweeted_status_user_id', 'retweeted_status_timestamp'. \n",
    "\n",
    "3. `tweet_archive Table`: missing data in 'in_reply_to_status_id', 'in_reply_to_user_id' and 'expanded_urls' columns\n",
    "\n",
    "4. `tweet_archive Table`: Validity issue in the names of the dogs - some names are *a*, *all*, *the*. It appears the inconsistent names start with lower cases. \n",
    "\n",
    "5. `tweet_archive Table`: Names in `name` column start with uppercase\n",
    "\n",
    "6. Inappropriate column datatypes:\n",
    "    - timestamp to datetime in `tweet_archive` table\n",
    "    - tweet_id in all 3 tables to string\n",
    "\n",
    "7. Column names not descriptive: \n",
    "    - img_num, p1, p1_conf, p1_dog, p2, p2_conf etc in `tweet_image` table,\n",
    "    - name in `tweet_archive` table\n",
    " \n",
    "8. None in doggo, floofer, pupper and puppo columns are null values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "col": 0,
        "height": 7,
        "hidden": false,
        "row": 40,
        "width": 12
       },
       "report_default": {
        "hidden": false
       }
      }
     }
    }
   },
   "source": [
    "##### Tidiness issues\n",
    "1. tweet_archive Table: doggo, floofer, pupper and puppo can be multiple variables stored in one column\n",
    "\n",
    "2. tweet_id is common to all three table, hence, all the data can be represented in one table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning\n",
    "Each of the issues identified were cleaned using the pandas library. The steps taken are defined below:\n",
    "* First, a copy of each dataframe were made so that the original dataframe remains unchanged and can always be referred to\n",
    "- From the **Tweet_Archive** copy of the DataFrame:\n",
    "    - The 181 retweeted tweets were identified and dropped from the DataFrame, leaving only original tweets in the dataset\n",
    "    - As a result of the step above, all columns containing retweeted data became empty, hence, they were dropped from the dataset\n",
    "    - The in-reply columns were also removed as they had over 90% missing data. No meaningful insights can be drawn from them\n",
    "    - The invalid names were identified and changed to \"None\". When checking the unique values in the name column, I discovered the invalid names started with lower cases because they were mostly verbs, conjunctions or adjectives while the valid names started with upper cases. This enabled me sort out the invalid names.\n",
    "    - I was then able to change all the names to lower case.\n",
    "    - None values in doggo, floofer, pupper and puppo columns were changed to null using the numpy _np.nan_\n",
    "- The columns with inapproriate datatype were also changed to the appropriate data type\n",
    "    - timestamp was changed to datetime data type\n",
    "    - tweet_id in all three tables were changed to object data type\n",
    "- From the **Tweet_image** copy of the DataFrame:\n",
    "    - Column names were changed to be more descriptive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The other issues were cleaned while dealing with the Tidiness Issues.\n",
    "- Firstly, the doggo, floofer, pupper, and puppo columns were collapsed into a multivariable column named dog_type.\n",
    "    - After collapsing, the number of observations increased because the observations were stacked on each other 4 times. To solve this, duplicated observations were dropped.\n",
    "    - Duplicated tweet_ids with None variable in the dog_type were also dropped in order to leave unique tweet_id data record. However, there were 12 duplicated tweet_ids which have two dogtype (in the sample in the code - **doggo** and **pupper**) records as extracted from the text of the tweets (These were left as is)\n",
    "* Secondly, all three tables were merged into one as they all have the tweet ids in common."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Storing\n",
    "After cleaning, the final dataframe was stored in a csv file - **twitter_archive_master.csv**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "vscode": {
   "interpreter": {
    "hash": "e437a2448c23dbc28e025c6eaec2ea7adfc4015fd60928410744ddc12b3746d2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
